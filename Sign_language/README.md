<h2>Sign_language Detection</h2> Uses <strong>lstm</strong> and <strong>ann</strong> to create a sequential model to detect the action/word that the user is trying to convey using sign_language. The lstm is trained on the dataset of different coordinates of various nodes and keypoints provided by the mediapipe .
<br>
<h4>Project Video<h4>

https://user-images.githubusercontent.com/55660103/183247537-feb565b6-29e9-43cd-aa89-3cede1abeca7.mp4
